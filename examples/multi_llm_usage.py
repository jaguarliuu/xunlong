"""å¤šç§å¤§æ¨¡å‹ä½¿ç”¨ç¤ºä¾‹"""

import asyncio
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from src.deep_search_agent import DeepSearchAgent
from src.llm import LLMConfig, LLMProvider, create_llm_config


async def test_different_providers():
    """æµ‹è¯•ä¸åŒçš„LLMæä¾›å•†"""
    print("=== å¤šç§å¤§æ¨¡å‹æä¾›å•†æµ‹è¯• ===\n")
    
    # æµ‹è¯•é…ç½®
    test_configs = []
    
    # é€šä¹‰åƒé—®
    if os.getenv("DASHSCOPE_API_KEY") or os.getenv("QWEN_API_KEY"):
        test_configs.append({
            "name": "é€šä¹‰åƒé—®",
            "provider": LLMProvider.QWEN,
            "model": "qwen-turbo",
            "api_key": os.getenv("DASHSCOPE_API_KEY") or os.getenv("QWEN_API_KEY")
        })
    
    # DeepSeek
    if os.getenv("DEEPSEEK_API_KEY"):
        test_configs.append({
            "name": "DeepSeek",
            "provider": LLMProvider.DEEPSEEK,
            "model": "deepseek-chat",
            "api_key": os.getenv("DEEPSEEK_API_KEY")
        })
    
    # æ™ºè°±AI
    if os.getenv("ZHIPU_API_KEY"):
        test_configs.append({
            "name": "æ™ºè°±AI",
            "provider": LLMProvider.ZHIPU,
            "model": "glm-4",
            "api_key": os.getenv("ZHIPU_API_KEY")
        })
    
    # OpenAI
    if os.getenv("OPENAI_API_KEY"):
        test_configs.append({
            "name": "OpenAI",
            "provider": LLMProvider.OPENAI,
            "model": "gpt-4o-mini",
            "api_key": os.getenv("OPENAI_API_KEY")
        })
    
    # Ollama (æœ¬åœ°)
    test_configs.append({
        "name": "Ollamaæœ¬åœ°",
        "provider": LLMProvider.OLLAMA,
        "model": "llama3",
        "api_key": None
    })
    
    if not test_configs:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„LLMé…ç½®")
        print("è¯·è®¾ç½®ç›¸åº”çš„ç¯å¢ƒå˜é‡:")
        print("  export DASHSCOPE_API_KEY='your-qwen-key'")
        print("  export DEEPSEEK_API_KEY='your-deepseek-key'")
        print("  export ZHIPU_API_KEY='your-zhipu-key'")
        print("  export OPENAI_API_KEY='your-openai-key'")
        return
    
    # æµ‹è¯•é—®é¢˜
    test_question = "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿè¯·ç”¨ä¸€å¥è¯å›ç­”ã€‚"
    
    for config in test_configs:
        print(f"--- æµ‹è¯• {config['name']} ---")
        
        try:
            # åˆ›å»ºè‡ªå®šä¹‰é…ç½®
            llm_config = create_llm_config(
                provider=config['provider'],
                model_name=config['model'],
                api_key=config['api_key'],
                temperature=0.7,
                max_tokens=100
            )
            
            # åˆ›å»ºæ™ºèƒ½ä½“ï¼ˆä½¿ç”¨è‡ªå®šä¹‰é…ç½®ï¼‰
            from src.llm.manager import LLMManager
            manager = LLMManager()
            manager.add_config("test", llm_config)
            
            client = manager.get_client("test")
            
            # æµ‹è¯•è¿æ¥
            print(f"ğŸ” æµ‹è¯•è¿æ¥...")
            answer = await client.simple_chat(test_question)
            
            print(f"âœ… {config['name']} å“åº”:")
            print(f"   {answer}")
            print(f"   æ¨¡å‹: {config['model']}")
            
        except Exception as e:
            print(f"âŒ {config['name']} æµ‹è¯•å¤±è´¥: {e}")
        
        print()


async def compare_model_responses():
    """æ¯”è¾ƒä¸åŒæ¨¡å‹çš„å“åº”"""
    print("=== æ¨¡å‹å“åº”æ¯”è¾ƒ ===\n")
    
    # åˆ›å»ºé»˜è®¤æ™ºèƒ½ä½“
    agent = DeepSearchAgent()
    
    # è·å–å¯ç”¨æä¾›å•†
    status = agent.get_system_status()
    available_providers = status['llm_manager']['available_providers']
    
    # ç­›é€‰å¯ç”¨çš„æä¾›å•†
    usable_providers = [
        name for name, info in available_providers.items() 
        if info['status'] == "å¯ç”¨"
    ]
    
    if not usable_providers:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„LLMæä¾›å•†")
        return
    
    print(f"ğŸ“Š æ¯”è¾ƒ {len(usable_providers)} ä¸ªå¯ç”¨æä¾›å•†çš„å“åº”")
    print(f"å¯ç”¨æä¾›å•†: {', '.join(usable_providers)}\n")
    
    # æµ‹è¯•é—®é¢˜
    questions = [
        "ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
        "Pythonå’ŒJavaçš„ä¸»è¦åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ",
        "å¦‚ä½•æé«˜å·¥ä½œæ•ˆç‡ï¼Ÿ"
    ]
    
    for i, question in enumerate(questions, 1):
        print(f"--- é—®é¢˜ {i}: {question} ---")
        
        for provider in usable_providers[:3]:  # é™åˆ¶æµ‹è¯•æ•°é‡
            try:
                # æ›´æ–°é…ç½®ä½¿ç”¨ç‰¹å®šæä¾›å•†
                manager = agent.llm_manager
                config = manager.get_config("default")
                
                # åˆ›å»ºä¸´æ—¶é…ç½®
                temp_config = create_llm_config(
                    provider=LLMProvider(provider),
                    model_name=available_providers[provider]['default_model'],
                    temperature=0.7,
                    max_tokens=150
                )
                
                manager.add_config(f"temp_{provider}", temp_config)
                client = manager.get_client(f"temp_{provider}")
                
                answer = await client.simple_chat(question)
                
                print(f"ğŸ¤– {provider}:")
                print(f"   {answer[:100]}{'...' if len(answer) > 100 else ''}")
                
            except Exception as e:
                print(f"âŒ {provider} å¤±è´¥: {e}")
        
        print()


async def test_agent_with_different_models():
    """æµ‹è¯•æ™ºèƒ½ä½“ä½¿ç”¨ä¸åŒæ¨¡å‹"""
    print("=== æ™ºèƒ½ä½“å¤šæ¨¡å‹æµ‹è¯• ===\n")
    
    # æµ‹è¯•ä¸åŒçš„æ™ºèƒ½ä½“é…ç½®
    test_queries = [
        "äººå·¥æ™ºèƒ½çš„å‘å±•å†å²",
        "åŒºå—é“¾æŠ€æœ¯çš„åº”ç”¨åœºæ™¯"
    ]
    
    for query in test_queries:
        print(f"ğŸ” æŸ¥è¯¢: {query}")
        
        try:
            # ä½¿ç”¨é»˜è®¤é…ç½®
            agent = DeepSearchAgent()
            
            # å¿«é€Ÿå›ç­”æµ‹è¯•
            answer = await agent.quick_answer(query)
            print(f"âœ… å¿«é€Ÿå›ç­”: {answer[:100]}...")
            
            # å¦‚æœæœ‰å¤šä¸ªå¯ç”¨æä¾›å•†ï¼Œå¯ä»¥æµ‹è¯•æ·±åº¦æœç´¢
            status = agent.get_system_status()
            available_count = sum(
                1 for info in status['llm_manager']['available_providers'].values()
                if info['status'] == "å¯ç”¨"
            )
            
            if available_count > 0:
                print("ğŸ” æ‰§è¡Œæ·±åº¦æœç´¢...")
                result = await agent.search(query, config={"timeout_seconds": 60})
                print(f"âœ… æœç´¢çŠ¶æ€: {result['status']}")
                
                if result.get('final_report'):
                    report = result['final_report']['report_content']
                    print(f"ğŸ“„ æŠ¥å‘Šé¢„è§ˆ: {report[:150]}...")
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        
        print("-" * 50)


async def main():
    """ä¸»å‡½æ•°"""
    print("DeepSearchå¤šç§å¤§æ¨¡å‹æ”¯æŒæ¼”ç¤º\n")
    
    try:
        # æµ‹è¯•ä¸åŒæä¾›å•†
        await test_different_providers()
        
        print("\n" + "="*60 + "\n")
        
        # æ¯”è¾ƒæ¨¡å‹å“åº”
        await compare_model_responses()
        
        print("\n" + "="*60 + "\n")
        
        # æµ‹è¯•æ™ºèƒ½ä½“
        await test_agent_with_different_models()
        
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        print(f"\næ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())