"""DeepSearch æµ‹è¯•è„šæœ¬"""

import asyncio
import json
from pathlib import Path

from src.config import DeepSearchConfig
from src.pipeline import DeepSearchPipeline


async def test_search():
    """æµ‹è¯•æœç´¢åŠŸèƒ½"""
    print("ğŸ” å¼€å§‹æµ‹è¯• DeepSearch...")
    
    # åˆ›å»ºé…ç½®
    config = DeepSearchConfig(
        headless=False,  # ä½¿ç”¨æœ‰å¤´æ¨¡å¼ä¾¿äºè§‚å¯Ÿ
        topk=3,          # åªæŠ“å–3ä¸ªç»“æœç”¨äºæµ‹è¯•
        shots_dir="./test_shots"
    )
    
    # åˆ›å»ºç®¡é“
    pipeline = DeepSearchPipeline(config)
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "Python web scraping",
        "æœºå™¨å­¦ä¹ å…¥é—¨",
        "2025å¹´äººå·¥æ™ºèƒ½å‘å±•è¶‹åŠ¿"
    ]
    
    for query in test_queries:
        print(f"\nğŸ“ æµ‹è¯•æŸ¥è¯¢: {query}")
        
        try:
            result = await pipeline.search(query)
            
            print(f"âœ… æœç´¢å®Œæˆ:")
            print(f"   - æ‰¾åˆ°ç»“æœ: {result.total_found}")
            print(f"   - æˆåŠŸå¤„ç†: {result.success_count}")
            print(f"   - å¤„ç†å¤±è´¥: {result.error_count}")
            print(f"   - æ‰§è¡Œæ—¶é—´: {result.execution_time:.2f}s")
            
            # ä¿å­˜ç»“æœ
            output_file = f"test_result_{query.replace(' ', '_')}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result.model_dump(), f, ensure_ascii=False, indent=2)
            
            print(f"   - ç»“æœå·²ä¿å­˜: {output_file}")
            
            # æ˜¾ç¤ºå‰2ä¸ªç»“æœçš„æ‘˜è¦
            for i, item in enumerate(result.items[:2], 1):
                status = "âœ“" if not item.error else "âœ—"
                print(f"   {status} {i}. {item.title[:50]}...")
                if item.text:
                    print(f"      æ­£æ–‡: {item.text[:100]}...")
                if item.screenshot_path:
                    print(f"      æˆªå›¾: {item.screenshot_path}")
            
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
        
        print("-" * 60)


async def test_single_page():
    """æµ‹è¯•å•é¡µé¢å¤„ç†"""
    print("\nğŸ” æµ‹è¯•å•é¡µé¢å¤„ç†...")
    
    config = DeepSearchConfig(headless=False)
    pipeline = DeepSearchPipeline(config)
    
    # æµ‹è¯•ä¸€ä¸ªç®€å•çš„æœç´¢
    result = await pipeline.search("GitHub")
    
    if result.items:
        item = result.items[0]
        print(f"âœ… é¡µé¢å¤„ç†æµ‹è¯•:")
        print(f"   URL: {item.url}")
        print(f"   æ ‡é¢˜: {item.title}")
        print(f"   æ­£æ–‡é•¿åº¦: {item.length} å­—ç¬¦")
        print(f"   æˆªå›¾: {item.screenshot_path}")
        print(f"   OGå›¾ç‰‡: {item.og_image_url}")
        print(f"   é¦–å›¾: {item.first_image_url}")
        
        if item.text:
            print(f"   æ­£æ–‡é¢„è§ˆ: {item.text[:200]}...")


if __name__ == "__main__":
    print("ğŸš€ DeepSearch æµ‹è¯•å¼€å§‹")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•ç›®å½•
    Path("test_shots").mkdir(exist_ok=True)
    
    try:
        # è¿è¡Œæµ‹è¯•
        asyncio.run(test_search())
        asyncio.run(test_single_page())
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()